{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ce060e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/reaction-workbench-env/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import logging\n",
    "import random\n",
    "import pkg_resources\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from rxnfp.tokenization import *\n",
    "# from rdkit import Chem\n",
    "# from rdkit.Chem import rdChemReactions\n",
    "torch.cuda.is_available()\n",
    "import rxnfp\n",
    "#from rxnfp.models import SmilesClassificationModel\n",
    "#from rxn_yields.core import SmilesTokenizer, SmilesClassificationModel\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "torch.cuda.is_available()\n",
    "# from simpletransformers.classification import ClassificationModel, ClassificationArgs\n",
    "from rxnfp.tokenization import SmilesTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "27a4dc45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: 3.8.8 | packaged by conda-forge | (default, Feb 20 2021, 16:22:27) \n",
      "[GCC 9.3.0]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(\"Python version:\", sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "df15a084",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.3.0+cu121\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"PyTorch version:\", torch.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "88b5cab6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/reaction-workbench-env/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel, AutoModelForSequenceClassification\n",
    "from rxnfp.tokenization import SmilesTokenizer\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "import argparse\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "def9a239",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('../ReactionClassification_2024/data/train.csv', delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b9b4277d",
   "metadata": {},
   "outputs": [],
   "source": [
    "val = pd.read_csv('../ReactionClassification_2024/data/val.csv', delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "94d8a4ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('../ReactionClassification_2024/data/test.csv', delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "626c244b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train1 = pd.read_csv('../ReactionClassification_2024/data/train1.csv', delimiter=',')\n",
    "train2 = pd.read_csv('../ReactionClassification_2024/data/train2.csv', delimiter=',')\n",
    "train3 = pd.read_csv('../ReactionClassification_2024/data/train3.csv', delimiter=',')\n",
    "train4 = pd.read_csv('../ReactionClassification_2024/data/train4.csv', delimiter=',')\n",
    "train5 = pd.read_csv('../ReactionClassification_2024/data/train5.csv', delimiter=',')\n",
    "train6 = pd.read_csv('../ReactionClassification_2024/data/train6.csv', delimiter=',')\n",
    "train7 = pd.read_csv('../ReactionClassification_2024/data/train7.csv', delimiter=',')\n",
    "train8 = pd.read_csv('../ReactionClassification_2024/data/train8.csv', delimiter=',')\n",
    "train9 = pd.read_csv('../ReactionClassification_2024/data/train9.csv', delimiter=',')\n",
    "train10 = pd.read_csv('../ReactionClassification_2024/data/train10.csv', delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f79bcd2",
   "metadata": {},
   "source": [
    "### Train Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c08b4b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mean Pooling - Take attention mask into account for correct averaging\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\n",
    "    sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "    return(sum_embeddings / sum_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "57b7e88e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = 'data/fine_tuned_model_Final1'\n",
    "#model_path = 'rxn-data-from-postgresql/models/reaxys_bert/checkpoint-4356432-epoch-36'\n",
    "\n",
    "model = AutoModel.from_pretrained(model_path, output_hidden_states=True)\n",
    "#tokenizer = SmilesTokenizer(vocab_file='rxn-data-from-postgresql/models/reaxys_bert/checkpoint-4235420-epoch-35/vocab.txt')\n",
    "tokenizer = SmilesTokenizer(vocab_file='data/fine_tuned_model_Final1/vocab.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cadbe137",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "val_sentence_embeddings = []\n",
    "test_sentence_embeddings = []\n",
    "batch= 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b2da12eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing train1: 100%|██████████| 3759/3759 [02:52<00:00, 21.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing train2: 100%|██████████| 3759/3759 [02:45<00:00, 22.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing train3: 100%|██████████| 3759/3759 [02:21<00:00, 26.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing train4: 100%|██████████| 3759/3759 [03:01<00:00, 20.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing train6: 100%|██████████| 3759/3759 [03:02<00:00, 20.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing train7: 100%|██████████| 3759/3759 [03:02<00:00, 20.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing train8:  53%|█████▎    | 1988/3759 [01:35<01:20, 21.91it/s]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for idx in range(1, 11):\n",
    "    print(idx)\n",
    "    train_df = globals()[f'train{idx}']\n",
    "    train_reactions = train_df['reaction'].astype(str).tolist()\n",
    "    sentence_embeddings = []\n",
    "    batch_size = 64  # Define a suitable batch size\n",
    "\n",
    "    for i in tqdm(range(0, len(train_reactions), batch_size), desc=f'Processing train{idx}'):\n",
    "        # Tokenize sentences\n",
    "        encoded_input = tokenizer(train_reactions[i:i+batch_size], padding=True, truncation=True, max_length=512, return_tensors='pt').to(device)\n",
    "\n",
    "        # Compute token embeddings\n",
    "        with torch.no_grad():\n",
    "            model_output = model(**encoded_input)\n",
    "\n",
    "        # Perform pooling\n",
    "        sentence_embeddings.extend([x.cpu().detach().numpy() for x in mean_pooling(model_output, encoded_input['attention_mask'])])\n",
    "\n",
    "    # Assign embeddings to the DataFrame and save to files\n",
    "    train_df['OptBertFP'] = sentence_embeddings\n",
    "    X_train = np.array(train_df['OptBertFP'])\n",
    "    np.save(f'../ReactionClassification_2024/data/OptBertFPFinal1/X_train{idx}_OptBertFP.npy', X_train)\n",
    "    #train_df.to_csv(f'../ReactionClassification_2024/data/PreBertFP/train{idx}_PreBertFP.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a076c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train.to_csv('data/PreBertFP/train_PreBertFP.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f78b6f9f",
   "metadata": {},
   "source": [
    "### Test and Val Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a873d892",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 3385/5175 [02:40<01:35, 18.68it/s]"
     ]
    }
   ],
   "source": [
    "val_reactions = val['reaction'].astype(str).tolist()\n",
    "for i in tqdm(range(0, len(val_reactions), batch)):\n",
    "    #Tokenize sentences\n",
    "    encoded_input = tokenizer(val_reactions[i:i+batch], padding=True, truncation=True, max_length=512, return_tensors='pt').to(device)\n",
    "\n",
    "    #Compute token embeddings\n",
    "    with torch.no_grad():\n",
    "        model_output = model(**encoded_input)\n",
    "\n",
    "    #Perform pooling. In this case, mean pooling\n",
    "    val_sentence_embeddings.extend([x.cpu().detach().numpy() for x in mean_pooling(model_output, encoded_input['attention_mask'])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b96d731d",
   "metadata": {},
   "outputs": [],
   "source": [
    "val['OptBertFP'] = val_sentence_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8166b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val = np.array(val['OptBertFP'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "572c7d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('../ReactionClassification_2024/data/OptBertFPFinal1/X_val_OptBertFP.npy', X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c18e5eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#val.to_csv('data/PreBertFP/val_PreBertFP.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db5e6504",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_reactions = test['reaction'].astype(str).tolist()\n",
    "for i in tqdm(range(0, len(test_reactions), batch)):\n",
    "    #Tokenize sentences\n",
    "    encoded_input = tokenizer(test_reactions[i:i+batch], padding=True, truncation=True, max_length=512, return_tensors='pt').to(device)\n",
    "\n",
    "    #Compute token embeddings\n",
    "    with torch.no_grad():\n",
    "        model_output = model(**encoded_input)\n",
    "\n",
    "    #Perform pooling. In this case, mean pooling\n",
    "    test_sentence_embeddings.extend([x.cpu().detach().numpy() for x in mean_pooling(model_output, encoded_input['attention_mask'])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b68b35b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "test['OptBertFP'] = test_sentence_embeddings\n",
    "X_test = np.array(test['OptBertFP'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e2a5527",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('../ReactionClassification_2024/data/OptBertFPFinal1/X_test_OptBertFP.npy', X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "22072059",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test.to_csv('data/PreBertFP/test_PreBertFP.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "928d4b79",
   "metadata": {},
   "source": [
    "### Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b9e5a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train1 = np.load('../ReactionClassification_2024/data/OptBertFPFinal1/X_train1_OptBertFP.npy',allow_pickle=True)\n",
    "X_train2 = np.load('../ReactionClassification_2024/data/OptBertFPFinal1/X_train2_OptBertFP.npy',allow_pickle=True)\n",
    "X_train3 = np.load('../ReactionClassification_2024/data/OptBertFPFinal1/X_train3_OptBertFP.npy',allow_pickle=True)\n",
    "X_train4 = np.load('../ReactionClassification_2024/data/OptBertFPFinal1/X_train4_OptBertFP.npy',allow_pickle=True)\n",
    "X_train5 = np.load('../ReactionClassification_2024/data/OptBertFPFinal1/X_train5_OptBertFP.npy',allow_pickle=True)\n",
    "X_train6 = np.load('../ReactionClassification_2024/data/OptBertFPFinal1/X_train6_OptBertFP.npy',allow_pickle=True)\n",
    "X_train7 = np.load('../ReactionClassification_2024/data/OptBertFPFinal1/X_train7_OptBertFP.npy',allow_pickle=True)\n",
    "X_train8 = np.load('../ReactionClassification_2024/data/OptBertFPFinal1/X_train8_OptBertFP.npy',allow_pickle=True)\n",
    "X_train9 = np.load('../ReactionClassification_2024/data/OptBertFPFinal1/X_train9_OptBertFP.npy',allow_pickle=True)\n",
    "X_train10 = np.load('../ReactionClassification_2024/data/OptBertFPFinal1/X_train10_OptBertFP.npy',allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "25cc94e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.concatenate([X_train1, X_train2, X_train3, X_train4, X_train5,\n",
    "                          X_train6, X_train7, X_train8, X_train9, X_train10], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c63e6b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train1 = np.vstack(X_train)\n",
    "X_train = np.squeeze(X_train1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "144f8ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train = np.load('data/RXNFP/X_train_RXNFP.npy')\n",
    "X_test = np.load('../ReactionClassification_2024/data/OptBertFPFinal1/X_test_OptBertFP.npy',allow_pickle=True)\n",
    "X_val = np.load('../ReactionClassification_2024/data/OptBertFPFinal1/X_val_OptBertFP.npy',allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ca5e0524",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test1 = np.vstack(X_test)\n",
    "X_test = np.squeeze(X_test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7b28fb0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val1 = np.vstack(X_val)\n",
    "X_val = np.squeeze(X_val1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d1f1e889",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('../ReactionClassification_2024/data/train.csv', delimiter=',')\n",
    "test = pd.read_csv('../ReactionClassification_2024/data/test.csv', delimiter=',')\n",
    "val = pd.read_csv('../ReactionClassification_2024/data/val.csv', delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ab494745",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train['CLASS-ID'].values\n",
    "y_test = test['CLASS-ID'].values\n",
    "y_val = val['CLASS-ID'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b238f4",
   "metadata": {},
   "source": [
    "### Test on Val set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "377c6c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "index = faiss.IndexFlatL2(X_train.shape[1])\n",
    "index.add(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "efa4358b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "k = 1 \n",
    "\n",
    "batch_size = 1000\n",
    "\n",
    "num_batches = (X_val.shape[0] + batch_size - 1) // batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aa0fbb01",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FAISS Search on Val Data: 100%|██████████| 332/332 [2:00:07<00:00, 21.71s/it]  \n"
     ]
    }
   ],
   "source": [
    "D_val_1nn = np.zeros((X_val.shape[0], k), dtype=np.float32)\n",
    "I_val_1nn = np.zeros((X_val.shape[0], k), dtype=np.int64)\n",
    "\n",
    "for b in tqdm(range(num_batches), desc='FAISS Search on Val Data'):\n",
    "    start = b * batch_size\n",
    "    end = min((b + 1) * batch_size, X_val.shape[0])\n",
    "    D, I = index.search(X_val[start:end], k)\n",
    "    D_val_1nn[start:end, :] = D\n",
    "    I_val_1nn[start:end, :] = I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "041d40c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_val_1nn = np.array(y_train[I_val_1nn])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "37908afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('../ReactionClassification_2024/data/OptBertFPFinal1/y_pred_val_1nn', y_pred_val_1nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5200d729",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_val_1nn = np.load('../ReactionClassification_2024/data/OptBertFPFinal1/y_pred_val_1nn.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a5397c91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Accuracy: 0.9611813479638023\n"
     ]
    }
   ],
   "source": [
    "print(f'Overall Accuracy: {accuracy_score(y_val, y_pred_val_1nn)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0040a85",
   "metadata": {},
   "source": [
    "### Test on Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "202ddae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 1  \n",
    "batch_size = 1000  \n",
    "\n",
    "num_batches = (X_test.shape[0] + batch_size - 1) // batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d643f735",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FAISS Search on Test Data: 100%|██████████| 331/331 [2:00:01<00:00, 21.76s/it]  \n"
     ]
    }
   ],
   "source": [
    "D_test_1nn = np.zeros((X_test.shape[0], k), dtype=np.float32)\n",
    "I_test_1nn = np.zeros((X_test.shape[0], k), dtype=np.int64)\n",
    "\n",
    "for b in tqdm(range(num_batches), desc='FAISS Search on Test Data'):\n",
    "    start = b * batch_size\n",
    "    end = min((b + 1) * batch_size, X_test.shape[0])\n",
    "    D, I = index.search(X_test[start:end], k)\n",
    "    D_test_1nn[start:end, :] = D\n",
    "    I_test_1nn[start:end, :] = I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "25ae6ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test_1nn = np.array(y_train[I_test_1nn])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "328b9a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('../ReactionClassification_2024/data/OptBertFPFinal1/y_pred_test_1nn', y_pred_test_1nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c13d19da",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test_1nn = np.load('../ReactionClassification_2024/data/OptBertFPFinal1/y_pred_test_1nn.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e6d14a6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Accuracy: 0.961824007738581\n"
     ]
    }
   ],
   "source": [
    "print(f'Overall Accuracy: {accuracy_score(y_test, y_pred_test_1nn)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a1379197",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/reaction-workbench-env/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "report_test1nn = classification_report(y_test, y_pred_test_1nn, output_dict=True)\n",
    "report_df = pd.DataFrame(report_test1nn).transpose()\n",
    "df_test1nn = report_df[:-3].reset_index().rename(columns={'index': 'CLASS-ID'})\n",
    "df_test1nn['test_support'] = df_test1nn['support'].astype(int)\n",
    "df_test1nn['CLASS-ID'] = df_test1nn['CLASS-ID'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1051893f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_class_support = train['CLASS-ID'].value_counts().sort_index()\n",
    "train_support_df = train_class_support.reset_index()\n",
    "train_support_df.columns = ['CLASS-ID', 'train_support']\n",
    "train_support_df['CLASS-ID'] = train_support_df['CLASS-ID'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4b514ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge with df_val1nn on 'CLASS-ID'\n",
    "df_test1nn_report = pd.merge(df_test1nn, train_support_df, on='CLASS-ID', how='left')\n",
    "# Sort based on the number of train_support\n",
    "df_test1nn_report = df_test1nn_report.sort_values(by='train_support', ascending=False)\n",
    "df_test1nn_report = df_test1nn_report.drop(columns=['support'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4ed0170e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_class = pd.read_csv('../ReactionClassification_2024/data/className.tsv', sep='\\t', encoding='ISO-8859-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e304ed77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert multi Class-ID into one\n",
    "def clean_class_id(row):\n",
    "    # Split the string by comma and convert to a list\n",
    "    class_ids = str(row['CLASS-ID']).split(',')\n",
    "    # Return the first element from the list, ensuring it's an integer\n",
    "    return int(class_ids[0].strip())\n",
    "\n",
    "# Apply the function to the 'CLASS-ID' column\n",
    "df_class['CLASS-ID'] = df_class.apply(clean_class_id, axis=1)\n",
    "df_class['CLASS-ID'] = df_class['CLASS-ID'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8336fc72",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test1nn = pd.merge(df_test1nn_report, df_class[['CLASS-ID', 'TRANSFORM_NAME', 'TRANSFORM_ID']], on='CLASS-ID', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e0ab9c31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CLASS-ID</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>test_support</th>\n",
       "      <th>train_support</th>\n",
       "      <th>TRANSFORM_NAME</th>\n",
       "      <th>TRANSFORM_ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1085</td>\n",
       "      <td>0.999199</td>\n",
       "      <td>0.999199</td>\n",
       "      <td>0.999199</td>\n",
       "      <td>24966</td>\n",
       "      <td>185024</td>\n",
       "      <td>Suzuki coupling, Suzuki-Miyaura Cross-Coupling</td>\n",
       "      <td>(ARCOUPLG)4.1.B, (AVNAMEDR)Suzuki-Miyaura</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>432</td>\n",
       "      <td>0.985259</td>\n",
       "      <td>0.987205</td>\n",
       "      <td>0.986231</td>\n",
       "      <td>21259</td>\n",
       "      <td>164914</td>\n",
       "      <td>N-alkylation of alkylamines</td>\n",
       "      <td>(AG2ALKN)1.1.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1016</td>\n",
       "      <td>0.994638</td>\n",
       "      <td>0.992842</td>\n",
       "      <td>0.993739</td>\n",
       "      <td>15507</td>\n",
       "      <td>113635</td>\n",
       "      <td>hydrolysis of carboxylic esters</td>\n",
       "      <td>(AQCLEAV1)1.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>60</td>\n",
       "      <td>0.999463</td>\n",
       "      <td>0.999106</td>\n",
       "      <td>0.999285</td>\n",
       "      <td>11185</td>\n",
       "      <td>85921</td>\n",
       "      <td>reduction of C-NO2 to C-NH2</td>\n",
       "      <td>(AAREDUCT)A.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>433</td>\n",
       "      <td>0.954267</td>\n",
       "      <td>0.957377</td>\n",
       "      <td>0.955820</td>\n",
       "      <td>7977</td>\n",
       "      <td>60554</td>\n",
       "      <td>N-alkylation of benzenoid amines, anilines</td>\n",
       "      <td>(AG2ALKN)1.1.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1294</th>\n",
       "      <td>1267</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>Brackeen Imidazole Synthesis</td>\n",
       "      <td>(AVNAMEDR)Brackeen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1295</th>\n",
       "      <td>1800</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>Shestakov Hydrazino Acid Synthesis</td>\n",
       "      <td>(AVNAMEDR)Shestakov</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1296</th>\n",
       "      <td>500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>O-propargylation of N-hydroxy amides, includin...</td>\n",
       "      <td>(AG2ALKO)2.4.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1297</th>\n",
       "      <td>1622</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>ListMacMillan Hydrogenation</td>\n",
       "      <td>(AVNAMEDR)List-MacMillan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1298</th>\n",
       "      <td>1404</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>Frankland-Duppa Reaction</td>\n",
       "      <td>(AVNAMEDR)Frankland-Duppa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1299 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      CLASS-ID  precision    recall  f1-score  test_support  train_support  \\\n",
       "0         1085   0.999199  0.999199  0.999199         24966         185024   \n",
       "1          432   0.985259  0.987205  0.986231         21259         164914   \n",
       "2         1016   0.994638  0.992842  0.993739         15507         113635   \n",
       "3           60   0.999463  0.999106  0.999285         11185          85921   \n",
       "4          433   0.954267  0.957377  0.955820          7977          60554   \n",
       "...        ...        ...       ...       ...           ...            ...   \n",
       "1294      1267   1.000000  1.000000  1.000000             1              8   \n",
       "1295      1800   1.000000  1.000000  1.000000             5              8   \n",
       "1296       500   0.000000  0.000000  0.000000             1              7   \n",
       "1297      1622   1.000000  1.000000  1.000000             1              7   \n",
       "1298      1404   1.000000  1.000000  1.000000             2              7   \n",
       "\n",
       "                                         TRANSFORM_NAME  \\\n",
       "0        Suzuki coupling, Suzuki-Miyaura Cross-Coupling   \n",
       "1                           N-alkylation of alkylamines   \n",
       "2                       hydrolysis of carboxylic esters   \n",
       "3                           reduction of C-NO2 to C-NH2   \n",
       "4            N-alkylation of benzenoid amines, anilines   \n",
       "...                                                 ...   \n",
       "1294                       Brackeen Imidazole Synthesis   \n",
       "1295                 Shestakov Hydrazino Acid Synthesis   \n",
       "1296  O-propargylation of N-hydroxy amides, includin...   \n",
       "1297                       ListMacMillan Hydrogenation   \n",
       "1298                           Frankland-Duppa Reaction   \n",
       "\n",
       "                                   TRANSFORM_ID  \n",
       "0     (ARCOUPLG)4.1.B, (AVNAMEDR)Suzuki-Miyaura  \n",
       "1                                (AG2ALKN)1.1.2  \n",
       "2                                 (AQCLEAV1)1.1  \n",
       "3                                 (AAREDUCT)A.1  \n",
       "4                                (AG2ALKN)1.1.3  \n",
       "...                                         ...  \n",
       "1294                         (AVNAMEDR)Brackeen  \n",
       "1295                        (AVNAMEDR)Shestakov  \n",
       "1296                             (AG2ALKO)2.4.1  \n",
       "1297                   (AVNAMEDR)List-MacMillan  \n",
       "1298                  (AVNAMEDR)Frankland-Duppa  \n",
       "\n",
       "[1299 rows x 8 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test1nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "453715e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test1nn.to_csv('data/OptBertFPFinal1/df_test1nn.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c840ed47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate macro average for precision, recall, and f1-score\n",
    "macro_precision2 = df_test1nn['precision'].mean()\n",
    "macro_recall2 = df_test1nn['recall'].mean()\n",
    "macro_f12 = df_test1nn['f1-score'].mean()\n",
    "\n",
    "# Calculate weighted average for precision, recall, and f1-score\n",
    "weighted_precision2 = (df_test1nn['precision'] * df_test1nn['test_support']).sum() / df_test1nn['test_support'].sum()\n",
    "weighted_recall2 = (df_test1nn['recall'] * df_test1nn['test_support']).sum() / df_test1nn['test_support'].sum()\n",
    "weighted_f12 = (df_test1nn['f1-score'] * df_test1nn['test_support']).sum() / df_test1nn['test_support'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "66dad2d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "output1 = f\"\"\"\n",
    "Macro-averages:\n",
    "- Precision: {macro_precision2:.4f}\n",
    "- Recall: {macro_recall2:.4f}\n",
    "- F1-score: {macro_f12:.4f}\n",
    "\n",
    "Weighted-averages:\n",
    "- Precision: {weighted_precision2:.4f}\n",
    "- Recall: {weighted_recall2:.4f}\n",
    "- F1-score: {weighted_f12:.4f}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "458af8a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Macro-averages:\n",
      "- Precision: 0.8882\n",
      "- Recall: 0.8757\n",
      "- F1-score: 0.8757\n",
      "\n",
      "Weighted-averages:\n",
      "- Precision: 0.9618\n",
      "- Recall: 0.9618\n",
      "- F1-score: 0.9616\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(output1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b4ffff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb68171b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "reaction-workbench-env [Python]",
   "language": "python",
   "name": "conda-env-reaction-workbench-env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
