{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bab4bd7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "import torch\n",
    "import logging\n",
    "import random\n",
    "import pkg_resources\n",
    "import sklearn\n",
    "from scipy.spatial.distance import euclidean\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "#from rxnfp.tokenization import *\n",
    "# from rdkit import Chem\n",
    "# from rdkit.Chem import rdChemReactions\n",
    "torch.cuda.is_available()\n",
    "#import rxnfp\n",
    "#from rxnfp.models import SmilesClassificationModel\n",
    "#from rxn_yields.core import SmilesTokenizer, SmilesClassificationModelÂ§1`\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "#torch.cuda.is_available()\n",
    "# from simpletransformers.classification import ClassificationModel, ClassificationArgs\n",
    "#from rxnfp.tokenization import SmilesTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8032e201",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel, AutoModelForSequenceClassification\n",
    "from rxnfp.tokenization import SmilesTokenizer\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "import argparse\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c3cf53b",
   "metadata": {},
   "source": [
    "### Train Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cbef0ec8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertModel were not initialized from the model checkpoint at ../Janssen_project/rxn-data-from-postgresql/models/reaxys_bert/checkpoint-4356432-epoch-36 and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel\n",
    "\n",
    "model_path = '../Janssen_project/rxn-data-from-postgresql/models/reaxys_bert/checkpoint-4356432-epoch-36'\n",
    "pretrained_model = AutoModel.from_pretrained(model_path, output_hidden_states=True)\n",
    "tokenizer = SmilesTokenizer(vocab_file='../Janssen_project/rxn-data-from-postgresql/models/reaxys_bert/checkpoint-4356432-epoch-36/vocab.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c9fb5e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_class = pd.read_csv('data/className.tsv', sep='\\t', encoding='ISO-8859-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "81589ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert multi Class-ID into one\n",
    "def clean_class_id(row):\n",
    "    # Split the string by comma and convert to a list\n",
    "    class_ids = str(row['CLASS-ID']).split(',')\n",
    "    # Return the first element from the list, ensuring it's an integer\n",
    "    return int(class_ids[0].strip())\n",
    "\n",
    "# Apply the function to the 'CLASS-ID' column\n",
    "df_class['CLASS-ID'] = df_class.apply(clean_class_id, axis=1)\n",
    "df_class['CLASS-ID'] = df_class['CLASS-ID'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6688fb10",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_class['SUPER_TRANSFORM_ID'] = df_class['TRANSFORM_ID'].str.extract(r'\\((.*?)\\)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "05436c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Name = pd.merge(train, df_class[['CLASS-ID', 'TRANSFORM_NAME', 'TRANSFORM_ID', 'SUPER_TRANSFORM_ID']], on='CLASS-ID', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9ae63cbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CLASS-ID</th>\n",
       "      <th>RX.ID</th>\n",
       "      <th>reaction</th>\n",
       "      <th>product_summary</th>\n",
       "      <th>TRANSFORM_NAME</th>\n",
       "      <th>TRANSFORM_ID</th>\n",
       "      <th>SUPER_TRANSFORM_ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>272</td>\n",
       "      <td>20308219</td>\n",
       "      <td>CC1(C)O[C@H]2[C@H]3C[C@@H]([C@H]2O1)C(=O)C3&gt;&gt;C...</td>\n",
       "      <td>CC1(C)O[C@H]2[C@H]3C[C@H](NC(=O)C3)[C@H]2O1</td>\n",
       "      <td>Beckmann ring expansion, Beckmann ring expansi...</td>\n",
       "      <td>(AEREARR)6.1, (AFCYCLIZ)2.1.2, (ATRNGSIZ)1.4, ...</td>\n",
       "      <td>AEREARR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>84</td>\n",
       "      <td>20308266</td>\n",
       "      <td>CC(=O)OC(\\C=C\\c1ccccc1)C#N&gt;&gt;CC(=O)OC(CC(=O)c1c...</td>\n",
       "      <td>CC(=O)OC(CC(=O)c1ccccc1)C#N</td>\n",
       "      <td>oxidation of alkenes to ketones</td>\n",
       "      <td>(ABOXIDAT)2.2.2</td>\n",
       "      <td>ABOXIDAT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>68</td>\n",
       "      <td>20308388</td>\n",
       "      <td>CC1(C)O[C@H]2[C@H](OC(=O)[C@H]2O1)[C@@H](O)CO&gt;...</td>\n",
       "      <td>C[C@H](O)[C@H]1OC(=O)[C@H]2OC(C)(C)O[C@@H]12</td>\n",
       "      <td>reductive cleavage of alkyl C-O to CH</td>\n",
       "      <td>(AAREDUCT)C.2.1</td>\n",
       "      <td>AAREDUCT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1872</td>\n",
       "      <td>20308549</td>\n",
       "      <td>O=Cc1ccccc1&gt;&gt;CC(C)NC(=S)c1ccccc1</td>\n",
       "      <td>CC(C)NC(=S)c1ccccc1</td>\n",
       "      <td>Willgerodt-Kindler rearrangement</td>\n",
       "      <td>(AVNAMEDR)Willgerodt</td>\n",
       "      <td>AVNAMEDR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>636</td>\n",
       "      <td>20308813</td>\n",
       "      <td>ClC(=O)c1ccccc1&gt;&gt;CCOC(=O)C(F)(F)[C@H](OC(=O)c1...</td>\n",
       "      <td>CCOC(=O)C(F)(F)[C@H](OC(=O)c1ccccc1)[C@H](O)CO</td>\n",
       "      <td>O-benzoylation of alcohols</td>\n",
       "      <td>(AG3ACYO)4.1</td>\n",
       "      <td>AG3ACYO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2405601</th>\n",
       "      <td>72</td>\n",
       "      <td>20308054</td>\n",
       "      <td>CCc1ccc2c3ccc4c(OC(C)=O)ccc([C@@H]5O[C@H](COC(...</td>\n",
       "      <td>CC(=O)OC[C@H]1O[C@H]([C@H](OC(C)=O)[C@@H]1OC(C...</td>\n",
       "      <td>oxidation of (CH2)2 to C=C</td>\n",
       "      <td>(ABOXIDAT)1.1.1</td>\n",
       "      <td>ABOXIDAT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2405602</th>\n",
       "      <td>42</td>\n",
       "      <td>20308093</td>\n",
       "      <td>CC1(C)C2CC1C(=C)CC2&gt;&gt;C[C@H]1CC[C@H]2C[C@@H]1C2...</td>\n",
       "      <td>C[C@H]1CC[C@H]2C[C@@H]1C2(C)C</td>\n",
       "      <td>reduction of non-conjugated alkenes</td>\n",
       "      <td>(AAREDUCT)4.3</td>\n",
       "      <td>AAREDUCT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2405603</th>\n",
       "      <td>42</td>\n",
       "      <td>20308094</td>\n",
       "      <td>CC1(C)C2CC1C(=C)CC2&gt;&gt;C[C@@H]1CC[C@H]2C[C@@H]1C...</td>\n",
       "      <td>C[C@@H]1CC[C@H]2C[C@@H]1C2(C)C</td>\n",
       "      <td>reduction of non-conjugated alkenes</td>\n",
       "      <td>(AAREDUCT)4.3</td>\n",
       "      <td>AAREDUCT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2405604</th>\n",
       "      <td>256</td>\n",
       "      <td>20308164</td>\n",
       "      <td>COC(=O)CCC[C@H](O)C=CBr&gt;&gt;COC(=O)CCC[C@H](O)C#C</td>\n",
       "      <td>COC(=O)CCC[C@H](O)C#C</td>\n",
       "      <td>=-X -H elimination to alkynes</td>\n",
       "      <td>(ADELIMIN)2.5</td>\n",
       "      <td>ADELIMIN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2405605</th>\n",
       "      <td>670</td>\n",
       "      <td>20308180</td>\n",
       "      <td>C[C@@H](OC(C)=O)\\C=C\\[C@H](C)OC(C)=O&gt;&gt;COC(=O)O...</td>\n",
       "      <td>COC(=O)O[C@@H](C)\\C=C\\[C@@H](C)OC(C)=O</td>\n",
       "      <td>O-oxycarbonylation of alcohols</td>\n",
       "      <td>(AG3ACYO)7.1.1</td>\n",
       "      <td>AG3ACYO</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2405606 rows Ã 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         CLASS-ID     RX.ID  \\\n",
       "0             272  20308219   \n",
       "1              84  20308266   \n",
       "2              68  20308388   \n",
       "3            1872  20308549   \n",
       "4             636  20308813   \n",
       "...           ...       ...   \n",
       "2405601        72  20308054   \n",
       "2405602        42  20308093   \n",
       "2405603        42  20308094   \n",
       "2405604       256  20308164   \n",
       "2405605       670  20308180   \n",
       "\n",
       "                                                  reaction  \\\n",
       "0        CC1(C)O[C@H]2[C@H]3C[C@@H]([C@H]2O1)C(=O)C3>>C...   \n",
       "1        CC(=O)OC(\\C=C\\c1ccccc1)C#N>>CC(=O)OC(CC(=O)c1c...   \n",
       "2        CC1(C)O[C@H]2[C@H](OC(=O)[C@H]2O1)[C@@H](O)CO>...   \n",
       "3                         O=Cc1ccccc1>>CC(C)NC(=S)c1ccccc1   \n",
       "4        ClC(=O)c1ccccc1>>CCOC(=O)C(F)(F)[C@H](OC(=O)c1...   \n",
       "...                                                    ...   \n",
       "2405601  CCc1ccc2c3ccc4c(OC(C)=O)ccc([C@@H]5O[C@H](COC(...   \n",
       "2405602  CC1(C)C2CC1C(=C)CC2>>C[C@H]1CC[C@H]2C[C@@H]1C2...   \n",
       "2405603  CC1(C)C2CC1C(=C)CC2>>C[C@@H]1CC[C@H]2C[C@@H]1C...   \n",
       "2405604     COC(=O)CCC[C@H](O)C=CBr>>COC(=O)CCC[C@H](O)C#C   \n",
       "2405605  C[C@@H](OC(C)=O)\\C=C\\[C@H](C)OC(C)=O>>COC(=O)O...   \n",
       "\n",
       "                                           product_summary  \\\n",
       "0              CC1(C)O[C@H]2[C@H]3C[C@H](NC(=O)C3)[C@H]2O1   \n",
       "1                              CC(=O)OC(CC(=O)c1ccccc1)C#N   \n",
       "2             C[C@H](O)[C@H]1OC(=O)[C@H]2OC(C)(C)O[C@@H]12   \n",
       "3                                      CC(C)NC(=S)c1ccccc1   \n",
       "4           CCOC(=O)C(F)(F)[C@H](OC(=O)c1ccccc1)[C@H](O)CO   \n",
       "...                                                    ...   \n",
       "2405601  CC(=O)OC[C@H]1O[C@H]([C@H](OC(C)=O)[C@@H]1OC(C...   \n",
       "2405602                      C[C@H]1CC[C@H]2C[C@@H]1C2(C)C   \n",
       "2405603                     C[C@@H]1CC[C@H]2C[C@@H]1C2(C)C   \n",
       "2405604                              COC(=O)CCC[C@H](O)C#C   \n",
       "2405605             COC(=O)O[C@@H](C)\\C=C\\[C@@H](C)OC(C)=O   \n",
       "\n",
       "                                            TRANSFORM_NAME  \\\n",
       "0        Beckmann ring expansion, Beckmann ring expansi...   \n",
       "1                          oxidation of alkenes to ketones   \n",
       "2                    reductive cleavage of alkyl C-O to CH   \n",
       "3                         Willgerodt-Kindler rearrangement   \n",
       "4                               O-benzoylation of alcohols   \n",
       "...                                                    ...   \n",
       "2405601                         oxidation of (CH2)2 to C=C   \n",
       "2405602                reduction of non-conjugated alkenes   \n",
       "2405603                reduction of non-conjugated alkenes   \n",
       "2405604                      =-X -H elimination to alkynes   \n",
       "2405605                     O-oxycarbonylation of alcohols   \n",
       "\n",
       "                                              TRANSFORM_ID SUPER_TRANSFORM_ID  \n",
       "0        (AEREARR)6.1, (AFCYCLIZ)2.1.2, (ATRNGSIZ)1.4, ...            AEREARR  \n",
       "1                                          (ABOXIDAT)2.2.2           ABOXIDAT  \n",
       "2                                          (AAREDUCT)C.2.1           AAREDUCT  \n",
       "3                                     (AVNAMEDR)Willgerodt           AVNAMEDR  \n",
       "4                                             (AG3ACYO)4.1            AG3ACYO  \n",
       "...                                                    ...                ...  \n",
       "2405601                                    (ABOXIDAT)1.1.1           ABOXIDAT  \n",
       "2405602                                      (AAREDUCT)4.3           AAREDUCT  \n",
       "2405603                                      (AAREDUCT)4.3           AAREDUCT  \n",
       "2405604                                      (ADELIMIN)2.5           ADELIMIN  \n",
       "2405605                                     (AG3ACYO)7.1.1            AG3ACYO  \n",
       "\n",
       "[2405606 rows x 7 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_Name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "926ad8b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Name = df_Name.drop(columns=\"product_summary\")\n",
    "df_Name = df_Name.drop(columns=\"RX.ID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e258169b",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = X_train \n",
    "df_Name['embedding'] = list(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5f0cecd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PairDataset:\n",
    "    def __init__(self, dataframe):\n",
    "        self.df = dataframe\n",
    "        self.pairs = []\n",
    "        self.targets = []\n",
    "        self.create_pairs()\n",
    "\n",
    "    def create_pairs(self):\n",
    "        positive_pairs = []\n",
    "        negative_pairs = []\n",
    "        positive_targets = []\n",
    "        negative_targets = []\n",
    "\n",
    "        # Generate positive pairs (label 0 for similar)\n",
    "        for _, group in tqdm(self.df.groupby('CLASS-ID'), desc='Positive Pairs'):\n",
    "            indices = group.index.tolist()\n",
    "            for i in range(len(indices)):\n",
    "                pair_index = (i + 1) % len(indices)\n",
    "                positive_pairs.append((indices[i], indices[pair_index]))\n",
    "                positive_targets.append(0)  # Positive pair (similar)\n",
    "\n",
    "        # Generate negative pairs (label 1 for dissimilar)\n",
    "        for _, super_group in tqdm(self.df.groupby('SUPER_TRANSFORM_ID'), desc='Negative Pairs'):\n",
    "            class_groups = list(super_group.groupby('CLASS-ID'))\n",
    "            super_transform_id = super_group['SUPER_TRANSFORM_ID'].iloc[0]\n",
    "\n",
    "            if super_transform_id != 'AVNAMEDR':  # For reactions not in AVNAMEDR\n",
    "                other_super_groups = [g for g in self.df.groupby('SUPER_TRANSFORM_ID') if g[0] != super_transform_id]\n",
    "                for class_group in class_groups:\n",
    "                    indices = class_group[1].index.tolist()\n",
    "                    if len(other_super_groups) > 0:\n",
    "                        for i in range(len(indices)):\n",
    "                            selected_super_group_index = np.random.choice(len(other_super_groups), 1)[0]\n",
    "                            selected_super_group = other_super_groups[selected_super_group_index]\n",
    "                            selected_super_group_indices = selected_super_group[1].index.tolist()\n",
    "                            if len(selected_super_group_indices) > 0:\n",
    "                                selected_index = np.random.choice(selected_super_group_indices, 1)[0]\n",
    "                                negative_pairs.append((indices[i], selected_index))\n",
    "                                negative_targets.append(1)  # Negative pair (dissimilar)\n",
    "            else:  # For reactions in AVNAMEDR\n",
    "                if len(class_groups) > 1:\n",
    "                    for class_group in class_groups:\n",
    "                        indices = class_group[1].index.tolist()\n",
    "                        other_class_groups = [g for g in class_groups if g[0] != class_group[0]]\n",
    "                        for i in range(len(indices)):\n",
    "                            if len(other_class_groups) > 0:\n",
    "                                selected_group_index = np.random.choice(len(other_class_groups), 1)[0]\n",
    "                                selected_group = other_class_groups[selected_group_index]\n",
    "                                selected_indices = selected_group[1].index.tolist()\n",
    "                                if len(selected_indices) > 0:\n",
    "                                    selected_index = np.random.choice(selected_indices, 1)[0]\n",
    "                                    negative_pairs.append((indices[i], selected_index))\n",
    "                                    negative_targets.append(1)  # Negative pair (dissimilar)\n",
    "\n",
    "        # Assign pairs and targets\n",
    "        self.pairs = positive_pairs + negative_pairs\n",
    "        self.targets = positive_targets + negative_targets\n",
    "\n",
    "    def print_sample_pairs_with_distances(self, num_samples):\n",
    "        from scipy.spatial.distance import euclidean\n",
    "\n",
    "        print(\"\\nSample Pairs with Distances:\")\n",
    "        sample_indices = np.random.choice(len(self.pairs), num_samples, replace=False)\n",
    "        for i, idx in enumerate(sample_indices):\n",
    "            pair = self.pairs[idx]\n",
    "            target = self.targets[idx]\n",
    "            embedding1 = self.df.loc[pair[0], 'embedding']\n",
    "            embedding2 = self.df.loc[pair[1], 'embedding']\n",
    "            distance = euclidean(embedding1, embedding2)\n",
    "\n",
    "            pair_type = \"Positive\" if target == 0 else \"Negative\"\n",
    "            print(f\"{pair_type} Pair {i + 1}:\")\n",
    "            print(f\"Reaction 1 ID: {pair[0]}\")\n",
    "            print(f\"Reaction 2 ID: {pair[1]}\")\n",
    "            print(f\"Distance: {distance:.4f}\")\n",
    "            print(\"-\" * 30)\n",
    "\n",
    "    def calculate_average_distances(self):\n",
    "        positive_distances = []\n",
    "        negative_distances = []\n",
    "\n",
    "        for idx, (index1, index2) in enumerate(self.pairs):\n",
    "            embedding1 = self.df.loc[index1, 'embedding']\n",
    "            embedding2 = self.df.loc[index2, 'embedding']\n",
    "            distance = euclidean(embedding1, embedding2)\n",
    "\n",
    "            if self.targets[idx] == 0:\n",
    "                positive_distances.append(distance)\n",
    "            else:\n",
    "                negative_distances.append(distance)\n",
    "\n",
    "        avg_positive_distance = np.mean(positive_distances)\n",
    "        avg_negative_distance = np.mean(negative_distances)\n",
    "\n",
    "        print(f\"Average Positive Pair Distance: {avg_positive_distance:.4f}\")\n",
    "        print(f\"Average Negative Pair Distance: {avg_negative_distance:.4f}\")\n",
    "\n",
    "    def plot_distance_distributions(self, num_samples=100):\n",
    "        positive_distances = []\n",
    "        negative_distances = []\n",
    "\n",
    "        sample_indices = np.random.choice(len(self.pairs), num_samples, replace=False)\n",
    "        for idx in sample_indices:\n",
    "            pair = self.pairs[idx]\n",
    "            target = self.targets[idx]\n",
    "            embedding1 = self.df.loc[pair[0], 'embedding']\n",
    "            embedding2 = self.df.loc[pair[1], 'embedding']\n",
    "            distance = euclidean(embedding1, embedding2)\n",
    "\n",
    "            if target == 0:\n",
    "                positive_distances.append(distance)\n",
    "            else:\n",
    "                negative_distances.append(distance)\n",
    "\n",
    "        plt.hist(positive_distances, bins=30, alpha=0.5, label='Positive Pairs')\n",
    "        plt.hist(negative_distances, bins=30, alpha=0.5, label='Negative Pairs')\n",
    "        plt.xlabel('Distance')\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.title('Distance Distribution of Positive and Negative Pairs')\n",
    "        plt.legend(loc='upper right')\n",
    "        plt.show()\n",
    "\n",
    "    def sanity_checks(self):\n",
    "        print(\"Performing sanity checks...\")\n",
    "        # Check if positive pairs have the same CLASS-ID\n",
    "        mismatch_count = 0\n",
    "        for idx, (index1, index2) in enumerate(self.pairs):\n",
    "            if self.targets[idx] == 0:  # Positive pair\n",
    "                if self.df.loc[index1, 'CLASS-ID'] != self.df.loc[index2, 'CLASS-ID']:\n",
    "                    mismatch_count += 1\n",
    "                    print(f\"Error: Positive pair with different CLASS_IDs found!\")\n",
    "                    print(f\"Pair indices: ({index1}, {index2})\")\n",
    "                    print(f\"CLASS-ID 1: {self.df.loc[index1, 'CLASS-ID']}, CLASS_ID 2: {self.df.loc[index2, 'CLASS-ID']}\")\n",
    "\n",
    "        assert mismatch_count == 0, \"Sanity check failed: Positive pairs with different CLASS_IDs found!\"\n",
    "\n",
    "        # Check if negative pairs have different CLASS-ID\n",
    "        mismatch_count = 0\n",
    "        for idx, (index1, index2) in enumerate(self.pairs):\n",
    "            if self.targets[idx] == 1:  # Negative pair\n",
    "                if self.df.loc[index1, 'CLASS-ID'] == self.df.loc[index2, 'CLASS-ID']:\n",
    "                    mismatch_count += 1\n",
    "                    print(f\"Error: Negative pair with the same CLASS_IDs found!\")\n",
    "                    print(f\"Pair indices: ({index1}, {index2})\")\n",
    "                    print(f\"CLASS-ID 1: {self.df.loc[index1, 'CLASS-ID']}, CLASS-ID 2: {self.df.loc[index2, 'CLASS-ID']}\")\n",
    "\n",
    "        assert mismatch_count == 0, \"Sanity check failed: Negative pairs with the same CLASS_IDs found!\"\n",
    "\n",
    "        print(f\"Total mismatches in positive pairs: {mismatch_count}\")\n",
    "        print(\"All sanity checks passed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "96b23582",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Positive Pairs: 100%|ââââââââââ| 1299/1299 [00:02<00:00, 605.11it/s] \n",
      "Negative Pairs: 100%|ââââââââââ| 34/34 [3:46:36<00:00, 399.89s/it]  \n"
     ]
    }
   ],
   "source": [
    "sample_fraction = 1  # Use 50% of the data\n",
    "sample_df = df_Name.sample(frac=sample_fraction, random_state=42)\n",
    "\n",
    "# Create the dataset and dataloaders\n",
    "dataset = PairDataset(sample_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6f148160",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing sanity checks...\n",
      "Total mismatches in positive pairs: 0\n",
      "All sanity checks passed!\n"
     ]
    }
   ],
   "source": [
    "dataset.sanity_checks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d5afe1a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of positive pairs: 2405606\n",
      "Total number of negative pairs: 2405606\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total number of positive pairs: {sum(1 for t in dataset.targets if t == 1)}\")\n",
    "print(f\"Total number of negative pairs: {sum(1 for t in dataset.targets if t == 0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5223debd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Positive Pair Distance: 2.3788\n",
      "Average Negative Pair Distance: 2.7412\n"
     ]
    }
   ],
   "source": [
    "dataset.calculate_average_distances()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7928d5c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.plot_distance_distributions(2405606)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f445e9a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReactionPairDataset(Dataset):\n",
    "    def __init__(self, dataframe, pairs, targets):\n",
    "        self.df = dataframe\n",
    "        self.pairs = pairs\n",
    "        self.targets = targets\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        index1, index2 = self.pairs[idx]\n",
    "        reaction1 = self.df.loc[index1, 'reaction']\n",
    "        reaction2 = self.df.loc[index2, 'reaction']\n",
    "        label = self.targets[idx]\n",
    "        return reaction1, reaction2, torch.tensor(label, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3b5d6093",
   "metadata": {},
   "outputs": [],
   "source": [
    "reaction_dataset = ReactionPairDataset(df_Name, dataset.pairs, dataset.targets)\n",
    "train_size = int(0.9 * len(reaction_dataset))\n",
    "val_size = len(reaction_dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(reaction_dataset, [train_size, val_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c6596cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5be724b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(987, 256, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 256)\n",
       "    (token_type_embeddings): Embedding(2, 256)\n",
       "    (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSdpaSelfAttention(\n",
       "            (query): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (key): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (value): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "pretrained_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8e459499",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(pretrained_model.parameters(), lr=1e-6) \n",
    "total_steps = len(train_dataloader) * 10  \n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8071a0a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContrastiveLoss(torch.nn.Module):\n",
    "    def __init__(self, margin=3):\n",
    "        super(ContrastiveLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, output1, output2, label):\n",
    "        euclidean_distance = torch.nn.functional.pairwise_distance(output1, output2)\n",
    "        loss = torch.mean((1 - label) * torch.pow(euclidean_distance, 2) +\n",
    "                          label * torch.pow(torch.clamp(self.margin - euclidean_distance, min=0.0), 2))\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c03d305e",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = ContrastiveLoss(margin=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5f605ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(dataloader):\n",
    "    pretrained_model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for reaction1, reaction2, target in dataloader:\n",
    "            inputs1 = tokenizer(reaction1, return_tensors='pt', padding=True, truncation=True, max_length=512).to(device)\n",
    "            inputs2 = tokenizer(reaction2, return_tensors='pt', padding=True, truncation=True, max_length=512).to(device)\n",
    "            target = target.to(device)\n",
    "            \n",
    "            embedding1 = pretrained_model(**inputs1).last_hidden_state[:, 0, :]\n",
    "            embedding2 = pretrained_model(**inputs2).last_hidden_state[:, 0, :]\n",
    "            \n",
    "            loss = criterion(embedding1, embedding2, target)\n",
    "            total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a8e29d78",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3:  37%|ââââ      | 49451/135316 [1:24:27<2:25:12,  9.86it/s, loss=5.95]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Epoch 1/3:  86%|âââââââââ | 116596/135316 [3:18:34<31:47,  9.81it/s, loss=4.28]  IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Epoch 2/3: 100%|ââââââââââ| 135316/135316 [3:50:12<00:00,  9.80it/s, loss=2.31]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/3, Training Loss: 0.5764485076330819, Validation Loss: 0.29443645152434544\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/3:  85%|âââââââââ | 115239/135316 [3:16:02<34:08,  9.80it/s, loss=1.94]  IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 3\n",
    "accumulation_steps = 4 \n",
    "#early_stopping_patience = 3\n",
    "best_val_loss = float('inf')\n",
    "#patience_counter = 0\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "for epoch in range(num_epochs):\n",
    "    pretrained_model.train()\n",
    "    total_loss = 0\n",
    "    progress_bar = tqdm(train_dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\", mininterval=1)  # Update every 100 seconds\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    for i, (reaction1, reaction2, target) in enumerate(progress_bar):\n",
    "        inputs1 = tokenizer(reaction1, return_tensors='pt', padding=True, truncation=True, max_length=512).to(device)\n",
    "        inputs2 = tokenizer(reaction2, return_tensors='pt', padding=True, truncation=True, max_length=512).to(device)\n",
    "        target = target.to(device)\n",
    "\n",
    "        with torch.cuda.amp.autocast():\n",
    "            embedding1 = pretrained_model(**inputs1).last_hidden_state[:, 0, :]\n",
    "            embedding2 = pretrained_model(**inputs2).last_hidden_state[:, 0, :]\n",
    "            loss = criterion(embedding1, embedding2, target)\n",
    "            loss = loss / accumulation_steps  # Scale the loss\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "        if (i + 1) % accumulation_steps == 0:\n",
    "            scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(pretrained_model.parameters(), max_norm=1.0)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        total_loss += loss.item() * accumulation_steps\n",
    "\n",
    "        # Print loss less frequently\n",
    "        if (i + 1) % (len(train_dataloader) // 10) == 0:\n",
    "            progress_bar.set_postfix(loss=total_loss / ((i + 1) // accumulation_steps if (i + 1) // accumulation_steps > 0 else 1))\n",
    "\n",
    "    val_loss = evaluate_model(val_dataloader)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Training Loss: {total_loss / len(train_dataloader)}, Validation Loss: {val_loss}\")\n",
    "\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e6a1323a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('data/fine_tuned_model_Final2/tokenizer_config.json',\n",
       " 'data/fine_tuned_model_Final2/special_tokens_map.json',\n",
       " 'data/fine_tuned_model_Final2/vocab.txt',\n",
       " 'data/fine_tuned_model_Final2/added_tokens.json')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrained_model.save_pretrained('data/fine_tuned_model_Final2')\n",
    "tokenizer.save_pretrained('data/fine_tuned_model_Final2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af767c2f",
   "metadata": {},
   "source": [
    "### Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9970e0df",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train1 = np.load('../ReactionClassification_2024/data/PreBertFP/X_train1_PreBertFP.npy',allow_pickle=True)\n",
    "X_train2 = np.load('../ReactionClassification_2024/data/PreBertFP/X_train2_PreBertFP.npy',allow_pickle=True)\n",
    "X_train3 = np.load('../ReactionClassification_2024/data/PreBertFP/X_train3_PreBertFP.npy',allow_pickle=True)\n",
    "X_train4 = np.load('../ReactionClassification_2024/data/PreBertFP/X_train4_PreBertFP.npy',allow_pickle=True)\n",
    "X_train5 = np.load('../ReactionClassification_2024/data/PreBertFP/X_train5_PreBertFP.npy',allow_pickle=True)\n",
    "X_train6 = np.load('../ReactionClassification_2024/data/PreBertFP/X_train6_PreBertFP.npy',allow_pickle=True)\n",
    "X_train7 = np.load('../ReactionClassification_2024/data/PreBertFP/X_train7_PreBertFP.npy',allow_pickle=True)\n",
    "X_train8 = np.load('../ReactionClassification_2024/data/PreBertFP/X_train8_PreBertFP.npy',allow_pickle=True)\n",
    "X_train9 = np.load('../ReactionClassification_2024/data/PreBertFP/X_train9_PreBertFP.npy',allow_pickle=True)\n",
    "X_train10 = np.load('../ReactionClassification_2024/data/PreBertFP/X_train10_PreBertFP.npy',allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7f66c15f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.concatenate([X_train1, X_train2, X_train3, X_train4, X_train5,\n",
    "                          X_train6, X_train7, X_train8, X_train9, X_train10], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ac02d0cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train1 = np.vstack(X_train)\n",
    "X_train = np.squeeze(X_train1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6267e64e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train = np.load('data/RXNFP/X_train_RXNFP.npy')\n",
    "X_test = np.load('../ReactionClassification_2024/data/PreBertFP/X_test_PreBertFP.npy',allow_pickle=True)\n",
    "X_val = np.load('../ReactionClassification_2024/data/PreBertFP/X_val_PreBertFP.npy',allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b3251e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test1 = np.vstack(X_test)\n",
    "X_test = np.squeeze(X_test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "65553be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val1 = np.vstack(X_val)\n",
    "X_val = np.squeeze(X_val1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "be46ad96",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('../ReactionClassification_2024/data/train.csv', delimiter=',')\n",
    "test = pd.read_csv('../ReactionClassification_2024/data/test.csv', delimiter=',')\n",
    "val = pd.read_csv('../ReactionClassification_2024/data/val.csv', delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "de712edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train['CLASS-ID'].values\n",
    "y_test = test['CLASS-ID'].values\n",
    "y_val = val['CLASS-ID'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb39e45",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "reaction-workbench-env [Python]",
   "language": "python",
   "name": "conda-env-reaction-workbench-env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
