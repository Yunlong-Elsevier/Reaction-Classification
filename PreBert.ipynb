{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b5264a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import logging\n",
    "import random\n",
    "import pkg_resources\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from rxnfp.tokenization import *\n",
    "# from rdkit import Chem\n",
    "# from rdkit.Chem import rdChemReactions\n",
    "torch.cuda.is_available()\n",
    "import rxnfp\n",
    "#from rxnfp.models import SmilesClassificationModel\n",
    "#from rxn_yields.core import SmilesTokenizer, SmilesClassificationModel\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "torch.cuda.is_available()\n",
    "# from simpletransformers.classification import ClassificationModel, ClassificationArgs\n",
    "from rxnfp.tokenization import SmilesTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0ce70bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel, AutoModelForSequenceClassification\n",
    "from rxnfp.tokenization import SmilesTokenizer\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "import argparse\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "import jupyterlab_widgets\n",
    "import ipywidgets\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9cb52254",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('../ReactionClassification_2024/data/train.csv', delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e02e88da",
   "metadata": {},
   "outputs": [],
   "source": [
    "val = pd.read_csv('../ReactionClassification_2024/data/val.csv', delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dfb58400",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('../ReactionClassification_2024/data/test.csv', delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eb3f6c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train1 = pd.read_csv('../ReactionClassification_2024/data/train1.csv', delimiter=',')\n",
    "train2 = pd.read_csv('../ReactionClassification_2024/data/train2.csv', delimiter=',')\n",
    "train3 = pd.read_csv('../ReactionClassification_2024/data/train3.csv', delimiter=',')\n",
    "train4 = pd.read_csv('../ReactionClassification_2024/data/train4.csv', delimiter=',')\n",
    "train5 = pd.read_csv('../ReactionClassification_2024/data/train5.csv', delimiter=',')\n",
    "train6 = pd.read_csv('../ReactionClassification_2024/data/train6.csv', delimiter=',')\n",
    "train7 = pd.read_csv('../ReactionClassification_2024/data/train7.csv', delimiter=',')\n",
    "train8 = pd.read_csv('../ReactionClassification_2024/data/train8.csv', delimiter=',')\n",
    "train9 = pd.read_csv('../ReactionClassification_2024/data/train9.csv', delimiter=',')\n",
    "train10 = pd.read_csv('../ReactionClassification_2024/data/train10.csv', delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ecf6520",
   "metadata": {},
   "source": [
    "### Train Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cad37570",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mean Pooling - Take attention mask into account for correct averaging\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\n",
    "    sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "    return(sum_embeddings / sum_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a96aae74",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertModel were not initialized from the model checkpoint at rxn-data-from-postgresql/models/reaxys_bert/checkpoint-4356432-epoch-36 and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_path = 'rxn-data-from-postgresql/models/reaxys_bert/checkpoint-4356432-epoch-36'\n",
    "#model_path = 'rxn-data-from-postgresql/models/reaxys_bert/checkpoint-4356432-epoch-36'\n",
    "\n",
    "model = AutoModel.from_pretrained(model_path, output_hidden_states=True)\n",
    "#tokenizer = SmilesTokenizer(vocab_file='rxn-data-from-postgresql/models/reaxys_bert/checkpoint-4235420-epoch-35/vocab.txt')\n",
    "tokenizer = SmilesTokenizer(vocab_file='rxn-data-from-postgresql/models/reaxys_bert/checkpoint-4356432-epoch-36/vocab.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "91d88790",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "val_sentence_embeddings = []\n",
    "test_sentence_embeddings = []\n",
    "batch= 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ef3c99ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e299abcc357d46b4bc2ea6fd38f845bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing train1:   0%|          | 0/3759 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "513d87a9b6ef404d9f69ab779d96f7b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing train2:   0%|          | 0/3759 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "769c0eae996e411e8f800e57f61786a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing train3:   0%|          | 0/3759 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e33878215ba41f384fb453950a6d8b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing train4:   0%|          | 0/3759 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e5b6cd08a4948f2ae16e4ddab591ae1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing train5:   0%|          | 0/3759 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a32b5f62ae794763b5072806aee361b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing train6:   0%|          | 0/3759 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4700a4c995447989dd666154b3254c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing train7:   0%|          | 0/3759 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "953fb9d4c1874a8aa158c336a8aa7663",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing train8:   0%|          | 0/3759 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6211fbbed498452e830050dba2e4f7c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing train9:   0%|          | 0/3759 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16466b7bea914bb8b69bfe4637afb0da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing train10:   0%|          | 0/3759 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for idx in range(1, 11):\n",
    "    print(idx)\n",
    "    train_df = globals()[f'train{idx}']\n",
    "    train_reactions = train_df['reaction'].astype(str).tolist()\n",
    "    sentence_embeddings = []\n",
    "    batch_size = 64  # Define a suitable batch size\n",
    "\n",
    "    for i in tqdm(range(0, len(train_reactions), batch_size), desc=f'Processing train{idx}'):\n",
    "        # Tokenize sentences\n",
    "        encoded_input = tokenizer(train_reactions[i:i+batch_size], padding=True, truncation=True, max_length=512, return_tensors='pt').to(device)\n",
    "\n",
    "        # Compute token embeddings\n",
    "        with torch.no_grad():\n",
    "            model_output = model(**encoded_input)\n",
    "\n",
    "        # Perform pooling\n",
    "        sentence_embeddings.extend([x.cpu().detach().numpy() for x in mean_pooling(model_output, encoded_input['attention_mask'])])\n",
    "\n",
    "    # Assign embeddings to the DataFrame and save to files\n",
    "    train_df['PreBertFP'] = sentence_embeddings\n",
    "    X_train = np.array(train_df['PreBertFP'])\n",
    "    np.save(f'../ReactionClassification_2024/data/PreBertFP/X_train{idx}_PreBertFP.npy', X_train)\n",
    "    #train_df.to_csv(f'../ReactionClassification_2024/data/PreBertFP/train{idx}_PreBertFP.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a42452",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train.to_csv('data/PreBertFP/train_PreBertFP.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7906162f",
   "metadata": {},
   "source": [
    "### Test and Val Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a2f0b31f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "777e4e2073d84009ad7127d4a092000c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5175 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val_reactions = val['reaction'].astype(str).tolist()\n",
    "for i in tqdm(range(0, len(val_reactions), batch)):\n",
    "    #Tokenize sentences\n",
    "    encoded_input = tokenizer(val_reactions[i:i+batch], padding=True, truncation=True, max_length=512, return_tensors='pt').to(device)\n",
    "\n",
    "    #Compute token embeddings\n",
    "    with torch.no_grad():\n",
    "        model_output = model(**encoded_input)\n",
    "\n",
    "    #Perform pooling. In this case, mean pooling\n",
    "    val_sentence_embeddings.extend([x.cpu().detach().numpy() for x in mean_pooling(model_output, encoded_input['attention_mask'])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b4fc50f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "val['PreBertFP'] = val_sentence_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5d696a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val = np.array(val['PreBertFP'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c0fd4fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('../ReactionClassification_2024/data/PreBertFP/X_val_PreBertFP.npy', X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6c45a74c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#val.to_csv('data/PreBertFP/val_PreBertFP.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f2031b09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "776e86a46d6a449aa5ebadcb207210ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5169 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_reactions = test['reaction'].astype(str).tolist()\n",
    "for i in tqdm(range(0, len(test_reactions), batch)):\n",
    "    #Tokenize sentences\n",
    "    encoded_input = tokenizer(test_reactions[i:i+batch], padding=True, truncation=True, max_length=512, return_tensors='pt').to(device)\n",
    "\n",
    "    #Compute token embeddings\n",
    "    with torch.no_grad():\n",
    "        model_output = model(**encoded_input)\n",
    "\n",
    "    #Perform pooling. In this case, mean pooling\n",
    "    test_sentence_embeddings.extend([x.cpu().detach().numpy() for x in mean_pooling(model_output, encoded_input['attention_mask'])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "64ba9f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "test['PreBertFP'] = test_sentence_embeddings\n",
    "X_test = np.array(test['PreBertFP'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c5643edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('../ReactionClassification_2024/data/PreBertFP/X_test_PreBertFP.npy', X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fdd82f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test.to_csv('data/PreBertFP/test_PreBertFP.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ed1cdea9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE\n"
     ]
    }
   ],
   "source": [
    "print('DONE')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ceb4b7",
   "metadata": {},
   "source": [
    "### Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4a25647e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train1 = np.load('../ReactionClassification_2024/data/PreBertFP/X_train1_PreBertFP.npy',allow_pickle=True)\n",
    "X_train2 = np.load('../ReactionClassification_2024/data/PreBertFP/X_train2_PreBertFP.npy',allow_pickle=True)\n",
    "X_train3 = np.load('../ReactionClassification_2024/data/PreBertFP/X_train3_PreBertFP.npy',allow_pickle=True)\n",
    "X_train4 = np.load('../ReactionClassification_2024/data/PreBertFP/X_train4_PreBertFP.npy',allow_pickle=True)\n",
    "X_train5 = np.load('../ReactionClassification_2024/data/PreBertFP/X_train5_PreBertFP.npy',allow_pickle=True)\n",
    "X_train6 = np.load('../ReactionClassification_2024/data/PreBertFP/X_train6_PreBertFP.npy',allow_pickle=True)\n",
    "X_train7 = np.load('../ReactionClassification_2024/data/PreBertFP/X_train7_PreBertFP.npy',allow_pickle=True)\n",
    "X_train8 = np.load('../ReactionClassification_2024/data/PreBertFP/X_train8_PreBertFP.npy',allow_pickle=True)\n",
    "X_train9 = np.load('../ReactionClassification_2024/data/PreBertFP/X_train9_PreBertFP.npy',allow_pickle=True)\n",
    "X_train10 = np.load('../ReactionClassification_2024/data/PreBertFP/X_train10_PreBertFP.npy',allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f3eb0d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.concatenate([X_train1, X_train2, X_train3, X_train4, X_train5,\n",
    "                          X_train6, X_train7, X_train8, X_train9, X_train10], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2318b646",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('../ReactionClassification_2024/data/PreBertFP/X_train_PreBertFP.npy', X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fa836dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train1 = np.vstack(X_train)\n",
    "X_train = np.squeeze(X_train1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "88fbcd3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train = np.load('data/RXNFP/X_train_RXNFP.npy')\n",
    "X_test = np.load('../ReactionClassification_2024/data/PreBertFP/X_test_PreBertFP.npy',allow_pickle=True)\n",
    "X_val = np.load('../ReactionClassification_2024/data/PreBertFP/X_val_PreBertFP.npy',allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "32e6c88c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test1 = np.vstack(X_test)\n",
    "X_test = np.squeeze(X_test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c135b651",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val1 = np.vstack(X_val)\n",
    "X_val = np.squeeze(X_val1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f6aa3970",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('../ReactionClassification_2024/data/train.csv', delimiter=',')\n",
    "test = pd.read_csv('../ReactionClassification_2024/data/test.csv', delimiter=',')\n",
    "val = pd.read_csv('../ReactionClassification_2024/data/val.csv', delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "df2f2a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train['CLASS-ID'].values\n",
    "y_test = test['CLASS-ID'].values\n",
    "y_val = val['CLASS-ID'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de3cac2",
   "metadata": {},
   "source": [
    "### Test on Val set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2e6b8717",
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "index = faiss.IndexFlatL2(X_train.shape[1])\n",
    "index.add(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6deda490",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "k = 1 \n",
    "\n",
    "batch_size = 1000\n",
    "\n",
    "num_batches = (X_val.shape[0] + batch_size - 1) // batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "efde9c11",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FAISS Search on Val Data: 100%|██████████| 332/332 [2:02:11<00:00, 22.08s/it]\n"
     ]
    }
   ],
   "source": [
    "D_val_1nn = np.zeros((X_val.shape[0], k), dtype=np.float32)\n",
    "I_val_1nn = np.zeros((X_val.shape[0], k), dtype=np.int64)\n",
    "\n",
    "for b in tqdm(range(num_batches), desc='FAISS Search on Val Data'):\n",
    "    start = b * batch_size\n",
    "    end = min((b + 1) * batch_size, X_val.shape[0])\n",
    "    D, I = index.search(X_val[start:end], k)\n",
    "    D_val_1nn[start:end, :] = D\n",
    "    I_val_1nn[start:end, :] = I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cf39742f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_val_1nn = np.array(y_train[I_val_1nn])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f4266124",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('../ReactionClassification_2024/data/PreBertFP/y_pred_val_1nn', y_pred_val_1nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8467d68a",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_val_1nn = np.load('../ReactionClassification_2024/data/PreBertFP/y_pred_val_1nn.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3f524e5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Accuracy: 0.8333056546118286\n"
     ]
    }
   ],
   "source": [
    "print(f'Overall Accuracy: {accuracy_score(y_val, y_pred_val_1nn)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81608138",
   "metadata": {},
   "source": [
    "### Test on Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e6771200",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 1  \n",
    "batch_size = 1000  \n",
    "\n",
    "num_batches = (X_test.shape[0] + batch_size - 1) // batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b01d59b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbcf306b034f4dd7a3750e080fe0ff4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FAISS Search on Test Data:   0%|          | 0/331 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "D_test_1nn = np.zeros((X_test.shape[0], k), dtype=np.float32)\n",
    "I_test_1nn = np.zeros((X_test.shape[0], k), dtype=np.int64)\n",
    "\n",
    "for b in tqdm(range(num_batches), desc='FAISS Search on Test Data'):\n",
    "    start = b * batch_size\n",
    "    end = min((b + 1) * batch_size, X_test.shape[0])\n",
    "    D, I = index.search(X_test[start:end], k)  \n",
    "    D_test_1nn[start:end, :] = D  \n",
    "    I_test_1nn[start:end, :] = I  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e23a57ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test_1nn = np.array(y_train[I_test_1nn])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7f70b273",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('../ReactionClassification_2024/data/PreBertFP/y_pred_test_1nn', y_pred_test_1nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ee21a7f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Accuracy: 0.8328043287687796\n"
     ]
    }
   ],
   "source": [
    "print(f'Overall Accuracy: {accuracy_score(y_test, y_pred_test_1nn)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9efd3334",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/reaction-workbench-env/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "report_test1nn = classification_report(y_test, y_pred_test_1nn, output_dict=True)\n",
    "report_df = pd.DataFrame(report_test1nn).transpose()\n",
    "df_test1nn = report_df[:-3].reset_index().rename(columns={'index': 'CLASS-ID'})\n",
    "df_test1nn['test_support'] = df_test1nn['support'].astype(int)\n",
    "df_test1nn['CLASS-ID'] = df_test1nn['CLASS-ID'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6445e1f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_class_support = train['CLASS-ID'].value_counts().sort_index()\n",
    "train_support_df = train_class_support.reset_index()\n",
    "train_support_df.columns = ['CLASS-ID', 'train_support']\n",
    "train_support_df['CLASS-ID'] = train_support_df['CLASS-ID'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2b508205",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge with df_val1nn on 'CLASS-ID'\n",
    "df_test1nn_report = pd.merge(df_test1nn, train_support_df, on='CLASS-ID', how='left')\n",
    "# Sort based on the number of train_support\n",
    "df_test1nn_report = df_test1nn_report.sort_values(by='train_support', ascending=False)\n",
    "df_test1nn_report = df_test1nn_report.drop(columns=['support'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "75888eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_class = pd.read_csv('../ReactionClassification_2024/data/className.tsv', sep='\\t', encoding='ISO-8859-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d2a77f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert multi Class-ID into one\n",
    "def clean_class_id(row):\n",
    "    # Split the string by comma and convert to a list\n",
    "    class_ids = str(row['CLASS-ID']).split(',')\n",
    "    # Return the first element from the list, ensuring it's an integer\n",
    "    return int(class_ids[0].strip())\n",
    "\n",
    "# Apply the function to the 'CLASS-ID' column\n",
    "df_class['CLASS-ID'] = df_class.apply(clean_class_id, axis=1)\n",
    "df_class['CLASS-ID'] = df_class['CLASS-ID'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "453b2dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test1nn = pd.merge(df_test1nn_report, df_class[['CLASS-ID', 'TRANSFORM_NAME', 'TRANSFORM_ID']], on='CLASS-ID', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "63f443d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CLASS-ID</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>test_support</th>\n",
       "      <th>train_support</th>\n",
       "      <th>TRANSFORM_NAME</th>\n",
       "      <th>TRANSFORM_ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1085</td>\n",
       "      <td>0.952258</td>\n",
       "      <td>0.977890</td>\n",
       "      <td>0.964904</td>\n",
       "      <td>24966</td>\n",
       "      <td>185024</td>\n",
       "      <td>Suzuki coupling, Suzuki-Miyaura Cross-Coupling</td>\n",
       "      <td>(ARCOUPLG)4.1.B, (AVNAMEDR)Suzuki-Miyaura</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>432</td>\n",
       "      <td>0.892553</td>\n",
       "      <td>0.912790</td>\n",
       "      <td>0.902558</td>\n",
       "      <td>21259</td>\n",
       "      <td>164914</td>\n",
       "      <td>N-alkylation of alkylamines</td>\n",
       "      <td>(AG2ALKN)1.1.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1016</td>\n",
       "      <td>0.885542</td>\n",
       "      <td>0.936480</td>\n",
       "      <td>0.910299</td>\n",
       "      <td>15507</td>\n",
       "      <td>113635</td>\n",
       "      <td>hydrolysis of carboxylic esters</td>\n",
       "      <td>(AQCLEAV1)1.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>60</td>\n",
       "      <td>0.906692</td>\n",
       "      <td>0.940009</td>\n",
       "      <td>0.923050</td>\n",
       "      <td>11185</td>\n",
       "      <td>85921</td>\n",
       "      <td>reduction of C-NO2 to C-NH2</td>\n",
       "      <td>(AAREDUCT)A.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>433</td>\n",
       "      <td>0.864972</td>\n",
       "      <td>0.874514</td>\n",
       "      <td>0.869717</td>\n",
       "      <td>7977</td>\n",
       "      <td>60554</td>\n",
       "      <td>N-alkylation of benzenoid amines, anilines</td>\n",
       "      <td>(AG2ALKN)1.1.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1294</th>\n",
       "      <td>1267</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>Brackeen Imidazole Synthesis</td>\n",
       "      <td>(AVNAMEDR)Brackeen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1295</th>\n",
       "      <td>1800</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>Shestakov Hydrazino Acid Synthesis</td>\n",
       "      <td>(AVNAMEDR)Shestakov</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1296</th>\n",
       "      <td>500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>O-propargylation of N-hydroxy amides, includin...</td>\n",
       "      <td>(AG2ALKO)2.4.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1297</th>\n",
       "      <td>1622</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>ListMacMillan Hydrogenation</td>\n",
       "      <td>(AVNAMEDR)List-MacMillan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1298</th>\n",
       "      <td>1404</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>Frankland-Duppa Reaction</td>\n",
       "      <td>(AVNAMEDR)Frankland-Duppa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1299 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      CLASS-ID  precision    recall  f1-score  test_support  train_support  \\\n",
       "0         1085   0.952258  0.977890  0.964904         24966         185024   \n",
       "1          432   0.892553  0.912790  0.902558         21259         164914   \n",
       "2         1016   0.885542  0.936480  0.910299         15507         113635   \n",
       "3           60   0.906692  0.940009  0.923050         11185          85921   \n",
       "4          433   0.864972  0.874514  0.869717          7977          60554   \n",
       "...        ...        ...       ...       ...           ...            ...   \n",
       "1294      1267   1.000000  1.000000  1.000000             1              8   \n",
       "1295      1800   0.000000  0.000000  0.000000             5              8   \n",
       "1296       500   0.000000  0.000000  0.000000             1              7   \n",
       "1297      1622   0.000000  0.000000  0.000000             1              7   \n",
       "1298      1404   0.000000  0.000000  0.000000             2              7   \n",
       "\n",
       "                                         TRANSFORM_NAME  \\\n",
       "0        Suzuki coupling, Suzuki-Miyaura Cross-Coupling   \n",
       "1                           N-alkylation of alkylamines   \n",
       "2                       hydrolysis of carboxylic esters   \n",
       "3                           reduction of C-NO2 to C-NH2   \n",
       "4            N-alkylation of benzenoid amines, anilines   \n",
       "...                                                 ...   \n",
       "1294                       Brackeen Imidazole Synthesis   \n",
       "1295                 Shestakov Hydrazino Acid Synthesis   \n",
       "1296  O-propargylation of N-hydroxy amides, includin...   \n",
       "1297                       ListMacMillan Hydrogenation   \n",
       "1298                           Frankland-Duppa Reaction   \n",
       "\n",
       "                                   TRANSFORM_ID  \n",
       "0     (ARCOUPLG)4.1.B, (AVNAMEDR)Suzuki-Miyaura  \n",
       "1                                (AG2ALKN)1.1.2  \n",
       "2                                 (AQCLEAV1)1.1  \n",
       "3                                 (AAREDUCT)A.1  \n",
       "4                                (AG2ALKN)1.1.3  \n",
       "...                                         ...  \n",
       "1294                         (AVNAMEDR)Brackeen  \n",
       "1295                        (AVNAMEDR)Shestakov  \n",
       "1296                             (AG2ALKO)2.4.1  \n",
       "1297                   (AVNAMEDR)List-MacMillan  \n",
       "1298                  (AVNAMEDR)Frankland-Duppa  \n",
       "\n",
       "[1299 rows x 8 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test1nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6b9abce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test1nn.to_csv('data/PreBertFP/df_test1nn.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "13a0b95f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate macro average for precision, recall, and f1-score\n",
    "macro_precision2 = df_test1nn['precision'].mean()\n",
    "macro_recall2 = df_test1nn['recall'].mean()\n",
    "macro_f12 = df_test1nn['f1-score'].mean()\n",
    "\n",
    "# Calculate weighted average for precision, recall, and f1-score\n",
    "weighted_precision2 = (df_test1nn['precision'] * df_test1nn['test_support']).sum() / df_test1nn['test_support'].sum()\n",
    "weighted_recall2 = (df_test1nn['recall'] * df_test1nn['test_support']).sum() / df_test1nn['test_support'].sum()\n",
    "weighted_f12 = (df_test1nn['f1-score'] * df_test1nn['test_support']).sum() / df_test1nn['test_support'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ba7a05b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "output1 = f\"\"\"\n",
    "Macro-averages:\n",
    "- Precision: {macro_precision2:.4f}\n",
    "- Recall: {macro_recall2:.4f}\n",
    "- F1-score: {macro_f12:.4f}\n",
    "\n",
    "Weighted-averages:\n",
    "- Precision: {weighted_precision2:.4f}\n",
    "- Recall: {weighted_recall2:.4f}\n",
    "- F1-score: {weighted_f12:.4f}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c9d20302",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Macro-averages:\n",
      "- Precision: 0.7646\n",
      "- Recall: 0.7221\n",
      "- F1-score: 0.7330\n",
      "\n",
      "Weighted-averages:\n",
      "- Precision: 0.8325\n",
      "- Recall: 0.8328\n",
      "- F1-score: 0.8319\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(output1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0975acb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d07d0e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "reaction-workbench-env [Python]",
   "language": "python",
   "name": "conda-env-reaction-workbench-env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
